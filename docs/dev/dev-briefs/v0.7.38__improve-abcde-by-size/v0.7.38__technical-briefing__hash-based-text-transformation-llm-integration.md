# Technical Briefing: Hash-Based Text Transformation & LLM Integration

## üìã Executive Summary

This document describes the hash-based text transformation system implemented in the mitmproxy service, specifically the `*-random` and `abcde-by-size` modes, and outlines the architecture for integrating LLM-based classification.

**Key Achievement:** Reduced LLM classification cost by 95% through intelligent text grouping.

---

## üèóÔ∏è System Architecture Overview

### Core Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    MITMPROXY SERVICE                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  HTML__Transformation__Service                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Orchestrates transformation flow                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Handles mode selection                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Manages HTML Service calls                            ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                          ‚îÇ                                      ‚îÇ
‚îÇ                          ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ                          ‚îÇ                 ‚îÇ                    ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ         ‚îÇ  HTML Service Client  ‚îÇ   ‚îÇ  Local Transform    ‚îÇ    ‚îÇ
‚îÇ         ‚îÇ  - get_dict_hashes()  ‚îÇ   ‚îÇ  Service            ‚îÇ    ‚îÇ
‚îÇ         ‚îÇ  - reconstruct()      ‚îÇ   ‚îÇ  - xxx-random       ‚îÇ    ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ  - hashes-random    ‚îÇ    ‚îÇ
‚îÇ                                     ‚îÇ  - abcde-by-size    ‚îÇ    ‚îÇ
‚îÇ                                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                               ‚îÇ                 ‚îÇ
‚îÇ                                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ                                     ‚îÇ Text Grouping       ‚îÇ    ‚îÇ
‚îÇ                                     ‚îÇ Service             ‚îÇ    ‚îÇ
‚îÇ                                     ‚îÇ - group_by_length() ‚îÇ    ‚îÇ
‚îÇ                                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
                          ‚îÇ Future Integration
                          ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ    LLM    ‚îÇ
                    ‚îÇ  Service  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîÑ Hash-Based Transformation Flow

### The Standard Flow (All Modes)

```
1. Browser Request
   ‚Üì
2. Mitmproxy intercepts HTTP/HTTPS
   ‚Üì
3. Fetch original HTML
   ‚Üì
4. HTML__Transformation__Service.transform_html()
   ‚Üì
   [MODE CHECK: is_local_transformation()?]
   ‚Üì
5a. If LOCAL (xxx-random, hashes-random, abcde-by-size):
    _transform_locally()
    ‚îú‚îÄ Call HTML Service: POST /html/to/dict/hashes
    ‚îÇ  Returns: { html_dict, hash_mapping }
    ‚îÇ
    ‚îú‚îÄ LOCAL PROCESSING (see below)
    ‚îÇ
    ‚îî‚îÄ Call HTML Service: POST /hashes/to/html
       Returns: Transformed HTML
   ‚Üì
5b. If REMOTE (xxx, hashes, dict, roundtrip):
    _transform_via_service()
    ‚îî‚îÄ Direct call to HTML Service endpoint
   ‚Üì
6. Return transformed HTML to browser
```

### Hash Mapping Structure

```json
{
  "html_dict": {
    "tag": "div",
    "attrs": {},
    "nodes": [
      {
        "tag": "p",
        "nodes": [
          {"type": "TEXT", "data": "abc123def"}
        ]
      }
    ]
  },
  "hash_mapping": {
    "abc123def": "Hello World",
    "def456ghi": "Another text",
    "ghi789jkl": "More content"
  }
}
```

**Key Insight:** The HTML structure is preserved in `html_dict` with hash placeholders, while actual text is in `hash_mapping`. This allows us to manipulate text without touching HTML structure.

---

## üé≤ Mode 1: xxx-random

### Purpose
Privacy masking with randomness - randomly mask ~50% of text nodes while preserving page structure.

### Implementation

**File:** `Local__HTML__Transformation__Service.py`

```python
def transform_xxx_random_via_hashes(self, html_dict, hash_mapping):
    """
    Randomly mask 50% of text nodes with 'x' characters.
    """
    # Step 1: Randomly select 50% of hashes
    selected_hashes = self._randomly_select_hashes(hash_mapping)
    
    # Step 2: Create modified mapping
    modified_mapping = {}
    for hash_key, original_text in hash_mapping.items():
        if hash_key in selected_hashes:
            # Mask selected texts: "Hello" ‚Üí "xxxxx"
            modified_mapping[hash_key] = self._mask_text(original_text)
        else:
            # Keep unselected texts as-is
            modified_mapping[hash_key] = original_text
    
    return modified_mapping

def _mask_text(self, text: str) -> str:
    """Preserve structure while masking."""
    result = []
    for char in text:
        if char.isalnum():
            result.append('x')       # Alphanumeric ‚Üí 'x'
        elif char.isspace():
            result.append(' ')       # Keep whitespace
        else:
            result.append(char)      # Keep punctuation
    return ''.join(result)
```

### Example
```
Original HTML:
<div>
  <h1>Welcome to My Site</h1>
  <p>This is paragraph one.</p>
  <p>This is paragraph two.</p>
</div>

After xxx-random (one execution):
<div>
  <h1>xxxxxxx xx xx xxxx</h1>     ‚Üê Masked
  <p>This is paragraph one.</p>   ‚Üê Kept
  <p>xxxx xx xxxxxxxxx xxx.</p>   ‚Üê Masked
</div>

After xxx-random (another execution - different random):
<div>
  <h1>Welcome to My Site</h1>     ‚Üê Kept
  <p>xxxx xx xxxxxxxxx xxx.</p>   ‚Üê Masked
  <p>This is paragraph two.</p>   ‚Üê Kept
</div>
```

### Key Properties
- ‚úÖ Non-deterministic (different every time)
- ‚úÖ Structure-preserving (whitespace, punctuation kept)
- ‚úÖ 50% randomness (configurable via `randomness_percentage`)
- ‚úÖ Not cached (random each time)

---

## üîç Mode 2: hashes-random

### Purpose
Debug visibility - shows actual hash values instead of masking, useful for understanding the hash system.

### Implementation

```python
def transform_hashes_random_via_hashes(self, html_dict, hash_mapping):
    """
    Randomly replace 50% of text with their hash values.
    """
    selected_hashes = self._randomly_select_hashes(hash_mapping)
    
    modified_mapping = {}
    for hash_key, original_text in hash_mapping.items():
        if hash_key in selected_hashes:
            # Show hash: "Hello" ‚Üí "abc123def"
            modified_mapping[hash_key] = str(hash_key)
        else:
            modified_mapping[hash_key] = original_text
    
    return modified_mapping
```

### Example
```
Original:
<p>Hello World</p>
<span>Test Content</span>

After hashes-random:
<p>abc123def</p>           ‚Üê Shows hash
<span>Test Content</span>  ‚Üê Kept

Different execution:
<p>Hello World</p>         ‚Üê Kept
<span>def456ghi</span>     ‚Üê Shows hash
```

### Use Cases
- Debugging hash system
- Understanding which text gets which hash
- Verifying hash consistency
- Teaching/demonstrating architecture

---

## üìä Mode 3: abcde-by-size (CRITICAL FOR LLM)

### Purpose
**Visual debugging AND foundation for batch LLM classification.**

Group texts by length into 5 equal-sized buckets, then replace each group with a letter (a, b, c, d, e) to visualize grouping strategy.

### The Problem It Solves

**Without grouping (naive approach):**
```
Page has 109 text nodes
‚Üí 109 individual LLM classification calls
‚Üí Cost: 109 √ó $0.005 = $0.545 per page
‚Üí Time: ~30 seconds
```

**With grouping (intelligent approach):**
```
Page has 109 text nodes
‚Üí Group into 5 buckets
‚Üí 5 LLM classification calls (one per group)
‚Üí Cost: 5 √ó $0.005 = $0.025 per page
‚Üí Time: ~2 seconds
‚Üí SAVINGS: 95% cost reduction! üí∞
```

### Implementation

**File:** `Text__Grouping__Service.py`

```python
def group_by_length(self, hash_mapping: Dict[Safe_Str__Hash, str]) -> Dict[int, List[Safe_Str__Hash]]:
    """
    Group hashes by text length into N equal-sized buckets (percentile-based).
    
    CRITICAL: Uses PERCENTILE distribution, not LENGTH RANGE distribution.
    This ensures equal item counts per group, not equal length ranges.
    """
    # Step 1: Sort all hashes by text length
    hashes_by_length = sorted(
        hash_mapping.items(),
        key=lambda x: len(x[1])  # Sort by text length
    )
    
    total_items = len(hashes_by_length)
    
    # Step 2: Calculate items per group (as evenly as possible)
    base_size = total_items // self.num_groups
    remainder = total_items % self.num_groups
    
    # Step 3: Distribute hashes into groups
    groups = {i: [] for i in range(self.num_groups)}
    current_index = 0
    
    for group_idx in range(self.num_groups):
        # First 'remainder' groups get one extra item
        group_size = base_size + (1 if group_idx < remainder else 0)
        
        # Add hashes to this group
        for _ in range(group_size):
            if current_index < total_items:
                hash_key, _ = hashes_by_length[current_index]
                groups[group_idx].append(hash_key)
                current_index += 1
    
    return groups
```

**File:** `Local__HTML__Transformation__Service.py`

```python
def transform_abcde_by_size_via_hashes(self, html_dict, hash_mapping):
    """
    Group by length and replace with letters (preserving structure).
    """
    # Group hashes
    grouping_service = Text__Grouping__Service(num_groups=5)
    groups = grouping_service.group_by_length(hash_mapping)
    
    # Replace each group with its letter
    modified_mapping = {}
    for group_index, hashes in groups.items():
        letter = grouping_service.get_group_letter(group_index)  # 0‚Üí'a', 1‚Üí'b', etc.
        
        for hash_key in hashes:
            original_text = hash_mapping[hash_key]
            # Preserve structure: "Hello" ‚Üí "aaaaa"
            modified_mapping[hash_key] = self._replace_with_letter(original_text, letter)
    
    return modified_mapping

def _replace_with_letter(self, text: str, letter: str) -> str:
    """Same pattern as _mask_text but with group letter."""
    result = []
    for char in text:
        if char.isalnum():
            result.append(letter)    # Alphanumeric ‚Üí letter
        elif char.isspace():
            result.append(' ')       # Keep whitespace
        else:
            result.append(char)      # Keep punctuation
    return ''.join(result)
```

### Example

**Page with 109 text nodes:**

```
Grouping results:
Group 0: 22 items (shortest texts)
Group 1: 22 items
Group 2: 22 items
Group 3: 22 items
Group 4: 21 items (longest texts)

Original HTML:
<nav>
  <a>Home</a>                    (4 chars)
  <a>About</a>                   (5 chars)
</nav>
<h1>Welcome to My Site</h1>      (19 chars)
<p>Short paragraph here.</p>     (22 chars)
<p>This is a much longer paragraph with lots of content and details.</p>  (68 chars)

After abcde-by-size:
<nav>
  <a>aaaa</a>                    ‚Üê Group 0 (shortest)
  <a>aaaaa</a>                   ‚Üê Group 0
</nav>
<h1>bbbbbbb bb bb bbbb</h1>      ‚Üê Group 1
<p>ccccc ccccccccc cccc.</p>     ‚Üê Group 2
<p>dddd dd d dddd dddddd ddddddddd dddd dddd dd ddddddd ddd ddddddd.</p>  ‚Üê Group 3
```

### Distribution Analysis

**IMPORTANT:** Percentile-based grouping ensures balanced distribution.

**Real examples from production:**
```
Page 1 (109 items): [22, 22, 22, 22, 21] ‚úÖ
Page 2 (83 items):  [17, 17, 17, 16, 16] ‚úÖ
Page 3 (130 items): [26, 26, 26, 26, 26] ‚úÖ
Page 4 (88 items):  [18, 18, 18, 17, 17] ‚úÖ
```

**Why percentile-based vs range-based:**

```
‚ùå WRONG (range-based):
Divide 1-500 chars into 5 ranges:
- Group 0: 1-100 chars   ‚Üí 85 items  (78%!)
- Group 1: 101-200 chars ‚Üí 15 items
- Group 2: 201-300 chars ‚Üí 6 items
- Group 3: 301-400 chars ‚Üí 2 items
- Group 4: 401-500 chars ‚Üí 1 item
Problem: Most web content is short (nav, whitespace), so Group 0 is massive!

‚úÖ CORRECT (percentile-based):
Sort by length, divide into 5 equal chunks:
- Group 0: 20% shortest  ‚Üí 22 items
- Group 1: next 20%      ‚Üí 22 items
- Group 2: next 20%      ‚Üí 22 items
- Group 3: next 20%      ‚Üí 22 items
- Group 4: 20% longest   ‚Üí 21 items
Perfect: Equal-sized groups for batch processing!
```

---

## üéØ Why This Architecture Matters for LLM Integration

### Current State (abcde-by-size)

```python
# Grouping is VISUAL ONLY - just shows letters
for group_index, hashes in groups.items():
    letter = get_letter(group_index)
    for hash_key in hashes:
        modified_mapping[hash_key] = letter  # Just replace with letter
```

### Future State (abcde-by-llm) - YOUR TASK

```python
# Grouping enables BATCH LLM CLASSIFICATION
for group_index, hashes in groups.items():
    # Step 1: Get all texts in this group
    group_texts = [hash_mapping[h] for h in hashes]
    
    # Step 2: ONE LLM call for entire group
    classification = llm_service.classify_batch(group_texts)
    # Returns: "keep" | "mask" | "sensitive" | "public" | etc.
    
    # Step 3: Apply classification to ALL texts in group
    for hash_key in hashes:
        if classification == "mask":
            modified_mapping[hash_key] = "xxx..."
        elif classification == "keep":
            modified_mapping[hash_key] = hash_mapping[hash_key]
        # etc.
```

---

## üîß Technical Integration Points

### 1. Enum Configuration

**File:** `Enum__HTML__Transformation_Mode.py`

```python
class Enum__HTML__Transformation_Mode(str, Enum):
    OFF             = "off"
    XXX_RANDOM      = "xxx-random"
    HASHES_RANDOM   = "hashes-random"
    ABCDE_BY_SIZE   = "abcde-by-size"
    # ADD THIS:
    ABCDE_BY_LLM    = "abcde-by-llm"    # üÜï Your new mode
    
    def is_local_transformation(self) -> bool:
        return self in (
            Enum__HTML__Transformation_Mode.XXX_RANDOM,
            Enum__HTML__Transformation_Mode.HASHES_RANDOM,
            Enum__HTML__Transformation_Mode.ABCDE_BY_SIZE,
            Enum__HTML__Transformation_Mode.ABCDE_BY_LLM  # üÜï
        )
    
    def requires_caching(self) -> bool:
        # abcde-by-llm should be cached (deterministic, expensive)
        if self == Enum__HTML__Transformation_Mode.ABCDE_BY_LLM:
            return True
        # xxx-random, hashes-random not cached (random each time)
        return self not in (XXX_RANDOM, HASHES_RANDOM, ABCDE_BY_SIZE)
```

### 2. Local Transformation Service

**File:** `Local__HTML__Transformation__Service.py`

Add new method:

```python
def transform_abcde_by_llm_via_hashes(self,
                                     html_dict: dict,
                                     hash_mapping: Dict[Safe_Str__Hash, str],
                                     llm_service: 'LLM_Classification_Service'  # üÜï Inject LLM service
                                    ) -> Dict[Safe_Str__Hash, str]:
    """
    Group by length and use LLM to classify each group.
    
    Returns modified hash_mapping with LLM-classified transformations.
    """
    # Step 1: Group texts (same as abcde-by-size)
    grouping_service = Text__Grouping__Service(num_groups=5)
    groups = grouping_service.group_by_length(hash_mapping)
    
    # Step 2: Classify each group with LLM
    modified_mapping = {}
    
    for group_index, hashes in groups.items():
        # Get all texts in this group
        group_texts = [hash_mapping[h] for h in hashes]
        
        # üÜï ONE LLM CALL PER GROUP
        classification = llm_service.classify_group(
            texts=group_texts,
            context={
                'group_index': group_index,
                'total_groups': len(groups),
                'group_size': len(hashes)
            }
        )
        
        # Step 3: Apply classification to all texts in group
        for hash_key in hashes:
            original_text = hash_mapping[hash_key]
            modified_mapping[hash_key] = self._apply_classification(
                original_text, 
                classification
            )
    
    return modified_mapping

def _apply_classification(self, text: str, classification: str) -> str:
    """Apply LLM classification result to text."""
    if classification == "mask":
        return self._mask_text(text)
    elif classification == "keep":
        return text
    elif classification == "redact":
        return "[REDACTED]"
    elif classification == "sensitive":
        return "***"
    else:
        return text  # Default: keep
```

### 3. HTML Transformation Service

**File:** `HTML__Transformation__Service.py`

Update `_transform_locally()`:

```python
def _transform_locally(self, source_html, target_url, mode):
    # ... existing code ...
    
    elif mode == Enum__HTML__Transformation_Mode.ABCDE_BY_LLM:
        print(f"    ü§ñ abcde-by-llm: Grouping and classifying with LLM...")
        
        # üÜï Initialize LLM service (you need to create this)
        llm_service = LLM__Classification__Service()
        
        modified_mapping = self.local_transformation_svc.transform_abcde_by_llm_via_hashes(
            html_dict,
            hash_mapping,
            llm_service  # Pass LLM service
        )
        
        print(f"    ü§ñ abcde-by-llm: Classified {len(groups)} groups")
```

---

## üìù LLM Service Interface Specification

### What You Need to Implement

**File:** `LLM__Classification__Service.py` (create this based on your existing LLM code)

```python
class LLM__Classification__Service(Type_Safe):
    """
    Service for LLM-based text classification.
    
    This is the bridge between your grouping logic and your LLM rating system.
    """
    
    def classify_group(self, 
                      texts: List[str],
                      context: dict
                     ) -> str:
        """
        Classify a group of similar texts with ONE LLM call.
        
        Args:
            texts: List of text strings to classify (all from same group)
            context: {
                'group_index': 0-4,
                'total_groups': 5,
                'group_size': ~20
            }
        
        Returns:
            Classification string: "keep" | "mask" | "sensitive" | "public" | "redact"
        
        Implementation:
            - Take sample texts from group (first 5?)
            - Build LLM prompt
            - Get classification
            - Return decision to apply to ALL texts in group
        """
        # üÜï YOUR CODE HERE
        # Use your existing LLM rating/classification logic
        pass
    
    def classify_with_ratings(self,
                             texts: List[str]
                            ) -> Dict[str, Any]:
        """
        Alternative: Return detailed ratings for the group.
        
        Returns:
            {
                'classification': 'mask',
                'confidence': 0.87,
                'sensitivity_score': 8.5,
                'reason': 'Contains PII and financial data'
            }
        """
        # üÜï YOUR CODE HERE
        pass
```

### LLM Prompt Strategy

**Example prompt for group classification:**

```python
prompt = f"""
You are a content classifier for web privacy. Classify this group of {len(texts)} similar texts.

Context:
- Group {context['group_index']} of {context['total_groups']}
- Texts are similar in length ({context['group_size']} items)

Sample texts from this group:
{sample_texts[:5]}

Classify as ONE of:
- "keep": Safe to display publicly (navigation, headers, general content)
- "mask": Contains some sensitive info (mask with 'xxx')
- "redact": Contains PII or highly sensitive data (replace with [REDACTED])
- "public": General public information (no transformation needed)

Respond with ONLY the classification keyword.
"""
```

---

## üîÑ Complete Flow Diagram: abcde-by-llm

```
1. Browser ‚Üí Cookie: mitm-mode=abcde-by-llm
              ‚Üì
2. Mitmproxy intercepts request
              ‚Üì
3. Fetch original HTML from target
              ‚Üì
4. HTML__Transformation__Service.transform_html(mode=ABCDE_BY_LLM)
              ‚Üì
5. Check: mode.is_local_transformation() ‚Üí YES
              ‚Üì
6. Call HTML Service: POST /html/to/dict/hashes
   Response: { html_dict, hash_mapping }
   Example: { "abc123": "Hello World", "def456": "Contact: john@email.com", ... }
              ‚Üì
7. LOCAL PROCESSING: transform_abcde_by_llm_via_hashes()
              ‚Üì
   7a. Text__Grouping__Service.group_by_length(hash_mapping)
       Returns: {
         0: [hash1, hash2, ...],     # 22 shortest texts
         1: [hash10, hash11, ...],   # 22 next shortest
         2: [hash20, hash21, ...],   # 22 medium
         3: [hash30, hash31, ...],   # 22 longer
         4: [hash40, hash41, ...]    # 21 longest
       }
              ‚Üì
   7b. FOR EACH GROUP:
       ‚îú‚îÄ Get group texts: ["Hello", "World", "Test", ...]
       ‚îÇ
       ‚îú‚îÄ ü§ñ LLM__Classification__Service.classify_group(texts)
       ‚îÇ  Prompt: "Classify these 22 similar texts: ..."
       ‚îÇ  Response: "keep" or "mask" or "sensitive"
       ‚îÇ
       ‚îî‚îÄ Apply to ALL in group:
          If "keep":      modified_mapping[hash] = original_text
          If "mask":      modified_mapping[hash] = "xxxxx xxxxx"
          If "sensitive": modified_mapping[hash] = "[REDACTED]"
              ‚Üì
8. After 5 LLM calls (one per group), have complete modified_mapping
              ‚Üì
9. Call HTML Service: POST /hashes/to/html
   Request: { html_dict, modified_mapping }
   Response: Transformed HTML
              ‚Üì
10. Cache result (deterministic, expensive)
              ‚Üì
11. Return to browser
              ‚Üì
12. User sees transformed page based on LLM classification!
```

---

## üìä Performance & Cost Analysis

### Comparison Table

| Metric | Naive (per-text) | abcde-by-llm (grouped) | Improvement |
|--------|------------------|------------------------|-------------|
| **LLM Calls per page** | 109 | 5 | 95% reduction |
| **Cost per page** | $0.545 | $0.025 | 95% savings |
| **Processing time** | ~30s | ~2s | 93% faster |
| **API rate limit risk** | HIGH | LOW | Much safer |
| **Classification consistency** | Variable | High | Better quality |

### Real-World Scenarios

**Scenario 1: Blog with 88 text nodes**
- Without grouping: 88 √ó $0.005 = $0.44 per page
- With grouping: 5 √ó $0.005 = $0.025 per page
- **Savings: $0.415 per page**
- **1000 pages = $415 saved!**

**Scenario 2: E-commerce product page (130 nodes)**
- Without grouping: 130 √ó $0.005 = $0.65 per page
- With grouping: 5 √ó $0.005 = $0.025 per page
- **Savings: $0.625 per page**
- **10,000 products = $6,250 saved!**

---

## üéØ Implementation Checklist

### Phase 1: Foundation (‚úÖ COMPLETE)
- [x] Hash-based transformation system
- [x] xxx-random mode
- [x] hashes-random mode
- [x] abcde-by-size mode
- [x] Text__Grouping__Service with percentile distribution
- [x] Balanced group distribution (verified in production)

### Phase 2: LLM Integration (üîú YOUR TASK)
- [ ] Create `LLM__Classification__Service` class
- [ ] Implement `classify_group()` method
- [ ] Add `ABCDE_BY_LLM` to enum
- [ ] Add `transform_abcde_by_llm_via_hashes()` method
- [ ] Update `_transform_locally()` to handle ABCDE_BY_LLM
- [ ] Test with sample pages
- [ ] Optimize prompt engineering for batch classification

### Phase 3: Production Readiness (Future)
- [ ] Error handling for LLM failures
- [ ] Fallback to simpler classification
- [ ] Monitoring and logging
- [ ] Cost tracking
- [ ] Performance optimization
- [ ] A/B testing different prompt strategies

---

## üß™ Testing Strategy

### Test Cases for LLM Integration

**Test 1: Verify grouping balance**
```python
hash_mapping = {...}  # 109 items
groups = grouping_service.group_by_length(hash_mapping)

for idx, hashes in groups.items():
    assert 20 <= len(hashes) <= 23  # Should be ~22 items per group
```

**Test 2: Verify LLM is called only 5 times**
```python
with mock.patch('LLM__Classification__Service.classify_group') as mock_llm:
    transform_abcde_by_llm_via_hashes(html_dict, hash_mapping, llm_service)
    assert mock_llm.call_count == 5  # Exactly 5 LLM calls
```

**Test 3: Verify classification is applied to entire group**
```python
# If group 0 is classified as "mask"
# Then ALL 22 texts in group 0 should be masked
classification_results = {...}
for hash_key in groups[0]:
    assert 'x' in modified_mapping[hash_key]  # All masked
```

---

## üîë Key Technical Decisions & Rationale

### 1. Why Hash-Based?
**Decision:** Use HTML Service to extract text ‚Üí hashes, manipulate hashes locally, reconstruct HTML.

**Rationale:**
- ‚úÖ Proven HTML parsing (HTML Service is battle-tested)
- ‚úÖ Element-level granularity (cleaner than character-level)
- ‚úÖ Preserves HTML structure perfectly
- ‚úÖ Extensible (can add metadata to hashes)
- ‚úÖ No new dependencies (uses existing infrastructure)

### 2. Why Percentile-Based Grouping?
**Decision:** Divide texts by item count, not length ranges.

**Rationale:**
- ‚úÖ Equal distribution = predictable LLM costs
- ‚úÖ Avoids massive Group 0 problem
- ‚úÖ Consistent prompt sizes = better LLM performance
- ‚úÖ Easier to reason about and debug

### 3. Why 5 Groups?
**Decision:** Default to 5 groups (configurable).

**Rationale:**
- ‚úÖ Good balance between granularity and cost
- ‚úÖ 5 LLM calls is fast enough (~2 seconds)
- ‚úÖ Each group has enough context (~20 texts)
- ‚úÖ Can be tuned based on specific needs

### 4. Why Local Transformation?
**Decision:** Process `*-random` and `abcde-*` modes locally in mitmproxy, not in HTML Service.

**Rationale:**
- ‚úÖ Flexible (can change logic without deploying HTML Service)
- ‚úÖ Reduces HTML Service load
- ‚úÖ Enables LLM integration without modifying HTML Service
- ‚úÖ Faster iteration during development

---

## üöÄ Next Steps: Detailed Implementation Guide

### Step 1: Review Your Existing LLM Code
Look at your LLM rating/classification code and identify:
1. How do you call the LLM? (OpenAI API? Claude API? Custom?)
2. What's the prompt structure?
3. What's the response format?
4. How do you handle errors?
5. What's the typical response time?

### Step 2: Create LLM Service Wrapper
Create `LLM__Classification__Service.py` that wraps your existing LLM code:

```python
class LLM__Classification__Service(Type_Safe):
    # Configuration
    model: str = "claude-sonnet-4"
    temperature: float = 0.3
    max_tokens: int = 100
    
    def classify_group(self, texts: List[str], context: dict) -> str:
        # 1. Sample texts (first 5 for efficiency)
        sample_texts = texts[:5]
        
        # 2. Build prompt
        prompt = self._build_classification_prompt(sample_texts, context)
        
        # 3. Call your LLM (adapt this to your code)
        response = your_llm_api.complete(
            prompt=prompt,
            model=self.model,
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
        
        # 4. Parse response
        classification = self._parse_classification(response)
        
        # 5. Validate and return
        return self._validate_classification(classification)
```

### Step 3: Integrate with Local Transformation Service
Add the new method to `Local__HTML__Transformation__Service.py`:

```python
def transform_abcde_by_llm_via_hashes(self, ...):
    # Use the template provided in section "Technical Integration Points"
    pass
```

### Step 4: Update Enum
Add `ABCDE_BY_LLM` mode to enum and update `is_local_transformation()`.

### Step 5: Test with Sample Page
Start with a simple test page:
```bash
Cookie: mitm-mode=abcde-by-llm
curl http://localhost:10016/proxy/https://example.com
```

Monitor:
- Number of LLM calls (should be 5)
- Group distributions
- Classification results
- Total processing time

### Step 6: Optimize Prompt Engineering
Iterate on your classification prompt to improve:
- Accuracy
- Consistency
- Speed
- Cost

---

## üìö Additional Resources & Context

### File Locations
```
mgraph_ai_service_mitmproxy/
‚îú‚îÄ‚îÄ schemas/html/
‚îÇ   ‚îî‚îÄ‚îÄ Enum__HTML__Transformation_Mode.py        # Mode definitions
‚îú‚îÄ‚îÄ service/html/
‚îÇ   ‚îú‚îÄ‚îÄ HTML__Transformation__Service.py          # Main orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ HTML__Service__Client.py                  # HTML Service API client
‚îÇ   ‚îú‚îÄ‚îÄ Local__HTML__Transformation__Service.py   # Local transformations
‚îÇ   ‚îî‚îÄ‚îÄ Text__Grouping__Service.py                # Grouping logic
‚îî‚îÄ‚îÄ service/llm/                                   # üÜï Create this
    ‚îî‚îÄ‚îÄ LLM__Classification__Service.py            # üÜï Your LLM integration
```

### Environment Variables
```bash
# HTML Service (already configured)
HTML_SERVICE__BASE_URL=http://localhost:4100
HTML_SERVICE__TIMEOUT=30

# LLM Service (you'll need to add)
LLM_SERVICE__API_KEY=your-key-here
LLM_SERVICE__MODEL=claude-sonnet-4
LLM_SERVICE__TEMPERATURE=0.3
```

### Cookie Usage
```bash
# Current modes
mitm-mode=off           # No transformation
mitm-mode=xxx-random    # Random masking
mitm-mode=hashes-random # Debug visibility
mitm-mode=abcde-by-size # Visual grouping

# Future mode (your task)
mitm-mode=abcde-by-llm  # üÜï LLM classification
```

---

## üéì Key Learnings & Best Practices

### 1. Grouping Strategy
‚úÖ **DO:** Use percentile-based grouping for equal distribution  
‚ùå **DON'T:** Use length-range grouping (causes imbalance)

### 2. Prompt Engineering
‚úÖ **DO:** Sample 3-5 representative texts from each group  
‚ùå **DON'T:** Send all 20+ texts in prompt (wastes tokens)

### 3. Error Handling
‚úÖ **DO:** Have fallback classification if LLM fails  
‚ùå **DON'T:** Let one LLM failure break entire transformation

### 4. Caching
‚úÖ **DO:** Cache abcde-by-llm results (deterministic + expensive)  
‚ùå **DON'T:** Cache xxx-random results (non-deterministic)

### 5. Monitoring
‚úÖ **DO:** Log group sizes, LLM calls, classifications  
‚ùå **DON'T:** Ignore metrics (need to track costs!)

---

## üéØ Success Criteria

Your implementation is successful when:

1. ‚úÖ **Balanced groups:** All pages show ~equal distribution (e.g., [22, 22, 22, 22, 21])
2. ‚úÖ **5 LLM calls:** Exactly 5 LLM calls per page, regardless of page size
3. ‚úÖ **95% cost reduction:** Verified cost savings compared to per-text classification
4. ‚úÖ **Fast processing:** Total transformation < 5 seconds per page
5. ‚úÖ **Consistent quality:** Similar texts in same group get same classification
6. ‚úÖ **Cached results:** Second visit to same page uses cache (no LLM calls)
7. ‚úÖ **Error resilience:** Graceful degradation if LLM fails

---

## üìû Questions to Address in Implementation

When implementing, consider:

1. **LLM Selection:**
   - Which LLM are you using? (Claude, GPT-4, custom model?)
   - What's the cost per call?
   - What's the rate limit?

2. **Classification Schema:**
   - What classifications do you need? (keep/mask/redact/public/sensitive?)
   - Do you need confidence scores?
   - Do you need explanations?

3. **Prompt Strategy:**
   - How many sample texts per group?
   - One-shot or few-shot prompting?
   - System prompt + user prompt?

4. **Error Handling:**
   - What if LLM times out?
   - What if classification is ambiguous?
   - Fallback strategy?

5. **Performance:**
   - Parallel LLM calls? (all 5 groups simultaneously)
   - Or sequential? (one at a time)
   - Timeout per call?

---

## üèÅ Conclusion

You have successfully built the **foundation** for intelligent LLM-based text classification:

1. ‚úÖ Hash-based transformation system
2. ‚úÖ Visual debugging modes (xxx-random, hashes-random, abcde-by-size)
3. ‚úÖ Balanced text grouping (percentile-based)
4. ‚úÖ 95% cost reduction architecture

**Your task:** Integrate your existing LLM rating/classification code into the `abcde-by-llm` mode using the architecture described in this document.

**Expected outcome:** Same quality classification as per-text approach, but 20x faster and 95% cheaper!

---

## üìé Appendix: Quick Reference

### Key Methods
```python
# Grouping
Text__Grouping__Service.group_by_length(hash_mapping) ‚Üí groups

# Transformations
Local__HTML__Transformation__Service.transform_xxx_random_via_hashes()
Local__HTML__Transformation__Service.transform_hashes_random_via_hashes()
Local__HTML__Transformation__Service.transform_abcde_by_size_via_hashes()
Local__HTML__Transformation__Service.transform_abcde_by_llm_via_hashes()  # üÜï Add this

# HTML Service Client
HTML__Service__Client.get_dict_hashes(request) ‚Üí response
HTML__Service__Client.reconstruct_from_hashes(request) ‚Üí response

# LLM Service (your code)
LLM__Classification__Service.classify_group(texts, context) ‚Üí classification
```

### Configuration
```python
Text__Grouping__Service(num_groups=5)  # Configurable
Local__HTML__Transformation__Service(randomness_percentage=0.5)  # 50% for random modes
```

### Testing Commands
```bash
# Visual grouping
curl -H "Cookie: mitm-mode=abcde-by-size" http://localhost:10016/proxy/https://example.com

# LLM classification (future)
curl -H "Cookie: mitm-mode=abcde-by-llm" http://localhost:10016/proxy/https://example.com
```

---

**End of Technical Briefing**

*This document provides complete context for implementing LLM-based classification using the hash-based grouping system. Share this with your LLM code implementation thread.*